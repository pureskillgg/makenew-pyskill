{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9ec32b",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "**FPS Critic Inc., owner of PureSkill.gg, is not liable for any AWS\n",
    "costs you incur. Only run this notebook if you understand and accept\n",
    "the AWS billing implications.**\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64833230",
   "metadata": {},
   "source": [
    "# Getting data from the ADX\n",
    "\n",
    "This notebook will help you download data from the main dataset off of the data exchange.\n",
    "\n",
    "Here are some definitions around the AWS Data Exchange (ADX):\n",
    "- Data Exchange: AWS service that hosts the data.\n",
    "- Data Product: Listing on the ADX where we publish the csds data.\n",
    "- Data Set: A container for one type of data.\n",
    "- Revision: For the csds data, each revision is equivalent to one day of data. This allows granularity for both data volume and date range.\n",
    "\n",
    "This notebook will transfer a selected set of revisions from the ADX to an S3 bucket on your AWS account. It will optionally allow you to enable automatic transfer of new revisions to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e40206",
   "metadata": {},
   "source": [
    "# You will incur costs!\n",
    "\n",
    "While the data set subscription as provided is free, **exporting and downloading the data set will incur a real money cost to your AWS bill according to their pricing**. Even if you are on the AWS free tier, the volume of data in this data set may exceed the free tier limits. We will summarize the expected costs, however we cannot guarantee these estimates. We recommend working with a few revisions in the data set to get real-world cost estimates for your use case.\n",
    "\n",
    "Things that will  cost money:\n",
    "- Exporting the data to S3 will generate PUT, COPY, POST, LIST requests.\n",
    "- Keeping the data in S3 will incur cost for storing objects.\n",
    "- If you setup auto-export of the data, you will get daily exports into your S3 bucket until you choose to disable the automatic process. This will incur increasing daily storage cost as the total data stored in your bucket will grow each day.\n",
    "- Downloading data from S3 across the internet, including to your local machine, will incur data transfer charges.\n",
    "- Exporting the data from the data exchange to a bucket outside of the region the data set is hosted in will incur additional data transfer charges.\n",
    "\n",
    "For example, let's compute the cost to export a revision to a bucket in in us-east-2 and download it to our local machine. Assume this revision has size 13GB, with 350 matches, and thus 11550 objects.\n",
    "\n",
    "- Exporting this data would incur 12k PUT requests for \\$0.06.\n",
    "- Since the data set is in us-east-1, and \n",
    "the bucket is in us-east-2, there is an additional data transfer cost of \\$0.13. \n",
    "This would cost nothing if the bucket was also in us-east-1.\n",
    "- Storing this data would cost \\$0.30 per month.\n",
    "- Downloading this data from S3 to you local machine would cost \\$1.17.\n",
    "- Thus it costs about \\$1.66 to download a single revision and store it on S3 for 1 month.\n",
    "https://calculator.aws/#/estimate?id=73fe611b111a0d2950d92ad2b10062b9cc01a91d\n",
    "\n",
    "You may use [this calculator](https://calculator.aws) to estimate your costs tailored to your account and region. For full S3 pricing information, refer to [the amazon pricing page](https://aws.amazon.com/s3/pricing/)\n",
    "\n",
    "We have included a calculator below to assist you in calculating your costs, however, we do not guarantee its accuracy. **You** are responsible for the final cost and payment of your AWS bill."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeba4c2",
   "metadata": {},
   "source": [
    "# Data Volume\n",
    "\n",
    "The volume of data and number of files is quite large for this dataset. For example, here are the number of matches in each month for the first five months:\n",
    "\n",
    "- December 2021: 17,952\n",
    "- January 2022: 15,315\n",
    "- February 2022: 9,892\n",
    "- March 2022: 16,855\n",
    "- April 2022: 8,717\n",
    "\n",
    "\n",
    "We can use the month of April to demonstrate the volume. The 30 revisions in April in total had:\n",
    "\n",
    "- 286,671 files\n",
    "- 8,717 matches\n",
    "- 275 GB\n",
    "\n",
    "These values are used in the calculator below to estimate costs on a per-revision basis. Though, the number of matches per day is variable.  For the first batch of historical revisions, you can find [the approximate number of matches for each day](https://docs.pureskill.gg/datascience/adx/csgo/csds/revisions). There is quite a large variance in some days due to spikes in user activity. The total number of matches in a revision will not exceed 1,600, however there may be more than one revision per day to accommodate additional volume.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4d737",
   "metadata": {},
   "source": [
    "# 1. Subscribe to the Dataset on the ADX\n",
    "\n",
    "The first step to getting the data is to subscribe to the [Product](https://aws.amazon.com/marketplace/pp/prodview-v3o7zrt6okwmo) on the ADX. \n",
    "\n",
    "Once your subscription has been approved, you can get your dataset ID by going to the ADX, clicking on \"Entitled data\", going to \"PureSkill.gg Competitive CS:GO Gameplay\", clicking on \"pureskillgg-csgo-production-dataexchange-csds-0\" under \"Entitled data sets\", expanding the \"Data set overview\" box, and copying the \"Data set ID\" on the right. Paste that into the `dataset_id` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a341d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"f49be2ef387af522a7b6f000158113e0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5161b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_id == \"f49be2ef387af522a7b6f000158113e0\":\n",
    "    raise Exception(\"Your dataset_id is still the default. Please subscribe on the ADX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603c1ae",
   "metadata": {},
   "source": [
    "# 2. Estimate Costs \n",
    "\n",
    "We provide the estimates below for convenience, but do not guarantee their accuracy or applicability to your AWS account. You should perform your own calculations using the cost calculator provided by AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "start_date=\"2022-04-01\"\n",
    "end_date=\"2022-05-01\"\n",
    "days_to_keep_data_on_s3 = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300805ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "revision_count = (date.fromisoformat(end_date) - date.fromisoformat(start_date)).days\n",
    "print('There are',revision_count,'revisions in that date range.')\n",
    "\n",
    "# From AWS\n",
    "cost_per_get = 0.0000004 # for US East (Virginia)\n",
    "cost_per_put = 0.000005 # for US East (Virginia)\n",
    "cost_per_gb_per_month = 0.023 # for US East (Virginia)\n",
    "cost_per_gb_transfer_from_adx = 0.00 # for US East (Virginia)\n",
    "cost_per_gb_transfer_out = 0.09 # for US East (Virginia)\n",
    "\n",
    "## Estimate for example EU region\n",
    "# cost_per_get = 0.00000042 # for EU (Paris)\n",
    "# cost_per_put = 0.0000053 # for EU (Paris)\n",
    "# cost_per_gb_per_month = 0.024 # for EU (Paris)\n",
    "# cost_per_gb_transfer_from_adx = 0.02 # for EU (Paris)\n",
    "# cost_per_gb_transfer_out = 0.09 # for EU (Paris)\n",
    "\n",
    "# # Estimate for example Asia Pacific region\n",
    "# cost_per_get = 0.00000035 # for Asia Pacific (Seoul)\n",
    "# cost_per_put = 0.0000045 # for Asia Pacific (Seoul)\n",
    "# cost_per_gb_per_month = 0.025 # for Asia Pacific (Seoul)\n",
    "# cost_per_gb_transfer_from_adx = 0.08 # for Asia Pacific (Seoul)\n",
    "# cost_per_gb_transfer_out = 0.11 # for Asia Pacific (Seoul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46091bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_per_revision = 8717/30 # April average\n",
    "# matches_per_revision = 9892/28 # Feb average\n",
    "GB_per_match = 296179095022/1024/1024/1024/8717 # very precise....\n",
    "files_per_match = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_month = 30\n",
    "\n",
    "number_of_matches = number_of_revisions*matches_per_revision\n",
    "number_of_files = number_of_matches*files_per_match\n",
    "storage_volume_GB = number_of_matches*GB_per_match\n",
    "\n",
    "storage_cost = storage_volume_GB*cost_per_gb_per_month*days_to_keep_data_on_s3/days_per_month\n",
    "\n",
    "transfer_put_cost = cost_per_put*number_of_files\n",
    "transfer_get_cost = cost_per_get*number_of_files\n",
    "\n",
    "transfer_volume_cost = storage_volume_GB*cost_per_gb_transfer_from_adx\n",
    "\n",
    "download_cost = storage_volume_GB*cost_per_gb_transfer_out\n",
    "\n",
    "print(f\"{number_of_revisions} revisions will be {round(storage_volume_GB,2)} GB, \"\n",
    "     f\"{number_of_matches} matches, and {number_of_files} files.\")\n",
    "print(f\"Cost to transfer to your S3 bucket: ${round(transfer_put_cost+transfer_volume_cost,3)}\")\n",
    "print(f\"Cost to store data in S3 for {days_to_keep_data_on_s3} days: ${round(storage_cost,3)}\")\n",
    "print(f\"Cost to transfer from S3 to local: ${round(transfer_get_cost+download_cost,3)}\")\n",
    "print(f\"Total cost: ${round(transfer_put_cost+transfer_volume_cost+storage_cost+transfer_get_cost+download_cost,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0e884",
   "metadata": {},
   "source": [
    "# 3. Understand the Consqeuences of your Actions\n",
    "\n",
    "Mark the variables true if you agree to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_understand_running_this_notebook_will_cost_money = False\n",
    "i_understand_the_estimates_above_may_be_wrong = False\n",
    "i_am_responsible_for_all_costs = False\n",
    "i_have_subscribed_to_the_dataset_on_the_adx = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc003b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    i_understand_running_this_notebook_will_cost_money\n",
    "    and i_understand_the_estimates_above_may_be_wrong\n",
    "    and i_am_responsible_for_those_costs\n",
    "    and i_have_subscribed_to_the_dataset_on_the_adx\n",
    "    ):\n",
    "    agreed_to_terms = True\n",
    "    print(\"Thank you for agreeing to the terms\")\n",
    "else:\n",
    "    agreed_to_terms = False\n",
    "    raise Exception(\"You have not accepted the terms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd1eef",
   "metadata": {},
   "source": [
    "## Import the usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc149e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pureskillgg_makenew_pyskill.notebook import setup_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ba17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4011bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import warnings\n",
    "import json\n",
    "from pureskillgg_dsdk import (\n",
    "    enable_auto_exporting_adx_dataset_revisions_to_s3,\n",
    "    disable_auto_exporting_adx_dataset_revisions_to_s3,\n",
    "    download_adx_dataset_revision,\n",
    "    get_adx_dataset_revisions,\n",
    "    export_single_adx_dataset_revision_to_s3,\n",
    "    export_multiple_adx_dataset_revisions_to_s3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d12df",
   "metadata": {},
   "source": [
    "# 4. Decide what to Transfer\n",
    "\n",
    "You must set **transfer_option** in the cell below. Here are the options and what they do:\n",
    "\n",
    "- none: Do not transfer any data.\n",
    "- range: Transfer a range of revisions from the ADX to your S3 bucket.\n",
    "- latest: Transfer the latest revision from the ADX to your S3 bucket.\n",
    "- all: Transfer all revisions from the ADX to your S3 bucket.\n",
    "- slowly-download-to-local: Transfer a range of revisions from the ADX to your local hard drive.\n",
    "\n",
    "To download a single revision, set the `start_date` and `end_date` to the same day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b35df",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "**FPS Critic Inc., owner of PureSkill.gg, is not liable for any AWS\n",
    "costs your incur. Run the cells below only if you understand and accept\n",
    "the AWS billing implications.**\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally set a bucket name. Otherwise, a bucket named [your-account-alias]-pureskillgg-dataset will be created.\n",
    "bucket = None\n",
    "prefix = None\n",
    "create_destination_bucket = True\n",
    "\n",
    "# This will set a lifecycle rule on the bucket if created via this notebook.\n",
    "# If the bucket already exists, no rule will be set.\n",
    "# Set to None for no expiration.\n",
    "days_to_keep_data_on_s3 = 30 \n",
    "\n",
    "# set based on what you want to transfer\n",
    "transfer_option = \"latest\"\n",
    "\n",
    "# set date range if you chose 'range' above\n",
    "start_date=\"2022-04-01\" # inclusive\n",
    "end_date=\"2022-05-01\" # exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec425f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if transfer_option == 'slowly-download-to-local':\n",
    "    warnings.warn(\"DOWNLOADING STRAIGHT TO LOCAL WILL BE SLOW. \"\n",
    "                  \"PERHAPS DAYS FOR A SINGLE REVISION. \"\n",
    "                  \"It will be 100x faster or more to transfer to S3 then download from there.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c093afd",
   "metadata": {},
   "source": [
    "## Create bucket, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003708a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def will_make_bucket(client, bucket, account_id):\n",
    "    try:\n",
    "        client.head_bucket(\n",
    "          Bucket=bucket,\n",
    "          ExpectedBucketOwner=account_id\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        err = str(e)\n",
    "        if '404' in err:\n",
    "            return True\n",
    "        if '403' in err:\n",
    "            raise Exception(f\"The bucket {bucket} exists and does not belong to you.\")\n",
    "    return False # already exists\n",
    "\n",
    "def get_bucket_name(bucket, account_id):\n",
    "    if bucket is not None:\n",
    "        return bucket\n",
    "    \n",
    "    account_aliases = boto3.client('iam').list_account_aliases()['AccountAliases']\n",
    "    if len(account_aliases) == 0:\n",
    "        account_alias = account_id\n",
    "    else:\n",
    "        account_alias = account_aliases[0]\n",
    "    return \"-\".join([account_alias, \"pureskillgg\", \"dataset\"])\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "bucket = get_bucket_name(bucket, account_id)\n",
    "\n",
    "if create_destination_bucket and agreed_to_terms:\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    \n",
    "    if will_make_bucket(client, bucket, account_id):\n",
    "        bucket_region = boto3.session.Session().region_name\n",
    "        print(f\"making bucket named {bucket} in {bucket_region}\")\n",
    "        \n",
    "        keywords = dict(\n",
    "            Bucket=bucket,\n",
    "            ACL=\"private\"\n",
    "            )\n",
    "        if bucket_region != 'us-east-1':\n",
    "            keywords['CreateBucketConfiguration']={\"LocationConstraint\": bucket_region}\n",
    "            \n",
    "        client.create_bucket(**keywords)\n",
    "\n",
    "        client.put_bucket_policy(\n",
    "            Bucket=bucket,\n",
    "            Policy=json.dumps(\n",
    "                {\n",
    "                    \"Version\": \"2012-10-17\",\n",
    "                    \"Statement\": [\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Principal\": {\"Service\": \"dataexchange.amazonaws.com\"},\n",
    "                            \"Action\": [\"s3:PutObject\", \"s3:PutObjectAcl\"],\n",
    "                            \"Resource\": f\"arn:aws:s3:::{bucket}/*\",\n",
    "                            \"Condition\": {\"StringEquals\": {\"aws:SourceAccount\": account_id}},\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        if days_to_keep_data_on_s3 is None:\n",
    "            client.put_bucket_lifecycle_configuration(\n",
    "                Bucket=bucket,\n",
    "                LifecycleConfiguration={\n",
    "                    \"Rules\": [\n",
    "                        {\n",
    "                            \"Expiration\": {\n",
    "                                \"Days\": days_to_keep_data_on_s3,\n",
    "                            },\n",
    "                            \"Status\": \"Enabled\",\n",
    "                            \"Filter\": {\"Prefix\": ''}\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            )\n",
    "        \n",
    "        client.put_public_access_block(\n",
    "            Bucket=bucket,\n",
    "            PublicAccessBlockConfiguration={\n",
    "                \"BlockPublicAcls\": True,\n",
    "                \"IgnorePublicAcls\": True,\n",
    "                \"BlockPublicPolicy\": True,\n",
    "                \"RestrictPublicBuckets\": True,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        print(f\"bucket {bucket} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50124c08",
   "metadata": {},
   "source": [
    "## Transfer latest revision from ADX to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe4e66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if transfer_option == \"latest\" and agreed_to_terms:\n",
    "    export_single_adx_dataset_revision_to_s3(\n",
    "        bucket, dataset_id, prefix=prefix\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a562d6a5",
   "metadata": {},
   "source": [
    "## Transfer date range from ADX to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if transfer_option == \"range\" and agreed_to_terms:\n",
    "    export_multiple_adx_dataset_revisions_to_s3(\n",
    "        bucket,\n",
    "        dataset_id,\n",
    "        prefix=prefix,\n",
    "        start_date=single_date,\n",
    "        end_date=single_date,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e0fad2",
   "metadata": {},
   "source": [
    "## Transfer everything from ADX to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if transfer_option == \"all\" and agreed_to_terms:\n",
    "    export_multiple_adx_dataset_revisions_to_s3(\n",
    "        bucket,\n",
    "        dataset_id,\n",
    "        prefix=prefix\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815be8f",
   "metadata": {},
   "source": [
    "## Transferring single revision from ADX to local (not recommended)\n",
    "\n",
    "This is not recommended because it will take a **VERY VERY** long time to download. Please instead download from your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if transfer_option == \"slowly-download-to-local\" and agreed_to_terms:\n",
    "    ds_collection_path = os.environ.get('PURESKILLGG_TOME_DS_COLLECTION_PATH')\n",
    "    download_adx_dataset_revision(ds_collection_path, dataset_id, prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0916ed1",
   "metadata": {},
   "source": [
    "# 5. Transferring Data from S3 to Local\n",
    "\n",
    "There are many many ways to do this so we won't list them all here. We generally sync one month at a time with the AWS CLI like this:\n",
    "\n",
    "```\n",
    "aws s3 sync s3://my-bucket/csds/2022/04/ /path/to/working/dir/csds/2022/04\n",
    "```\n",
    "\n",
    "It is **CRITICALLY IMPORTANT** to maintain the path structure. In the example above, you could change the `/path/to/working/dir` but the `csds/2022/04` is not something that you may change. The reader will not work because the folder structure is part of the key used to read each file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f6bc2",
   "metadata": {},
   "source": [
    "# 5. Automatically Exporting New Revisions to S3\n",
    "\n",
    "Finally, you can set a job to automatically export new revisions to your S3 bucket. Change the variables below to true to run the corresponding commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19a5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable automatic exporting of revisions to S3\n",
    "enable_automatic_revision_transfer = False\n",
    "disable_automatic_revision_transfer = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1723a5d",
   "metadata": {},
   "source": [
    "## Enable exporting of revisions to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812acb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_automatic_revision_transfer and agreed_to_terms:\n",
    "    disable_auto_exporting_adx_dataset_revisions_to_s3(dataset_id)\n",
    "    enable_auto_exporting_adx_dataset_revisions_to_s3(bucket, dataset_id, prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2236ff45",
   "metadata": {},
   "source": [
    "## Disable exporting of revisions to S3\n",
    "\n",
    "You can cancel your automatic exporting of revisions by running the code below. \n",
    "\n",
    "You can also disable this automatic job through the AWS console by:\n",
    "\n",
    "1. Navigating to the ADX\n",
    "1. Clicking on \"Entitled data\" under \"My Subscriptions\"\n",
    "1. Click on \"PureSkill.gg Competitive CS:GO Gameplay\"\n",
    "1. Click on the data set named \"pureskillgg-csgo-production-dataexchange-csds-0\"\n",
    "1. Look under \"Auto-export job destinations\" and you should be able to Remove any outstanding jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12664988",
   "metadata": {},
   "outputs": [],
   "source": [
    "if disable_automatic_revision_transfer and agreed_to_terms:\n",
    "    disable_auto_exporting_adx_dataset_revisions_to_s3(dataset_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
